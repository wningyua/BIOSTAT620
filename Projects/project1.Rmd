---
title: "Project1"
author: "Ningyuan Wang"
date: "3/4/2020"
output: word_document
fontsize: 12pt
geometry: margin=2in
header-included:
   - \usepackage{setspace}
   - \doublespacing

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(grid)
library(gridExtra)
library(dplyr)
```


```{r, basic setting}
# define time slot length in seconds, here is 60s.
slot_len <- 60 *10 # define slot length is 10min
session_path <- "~/Desktop/BIOSTAT620/Project1"
timezone <- 'America/Detroit'
var_name <- c('HR', 'ACC', 'TEMP', 'EDA')
# frequency
var_frac <- c(1,32, 4,4)
```


HR FACTS:
1. The heart rate (HR) is derived from the raw Blood Volume Pulse (BVP) signal by measuring the inter-beat interval (IBI). A normal heart rate during relaxation usually ranges between 50-70 beats per minute.
2. During sleep a slow heartbeat with rates around 40–50 bpm is common and is considered normal. When the heart is not beating in a regular pattern, this is referred to as an arrhythmia. Abnormalities of heart rate sometimes indicate disease.


```{r, function:add epoch time to each recording}
# day1
add_time_1 <- function(session_path){
  E4 <- list()
  setwd(session_path)
  for(i in 1:length(var_name)){
    # starting time
    start <- as.numeric(unlist(strsplit(readLines(paste(var_name[i], '1.csv', sep = ''),n=1), split = ","))[1])
    if(var_name[i] == 'ACC'){
      # ACC data have 3 columns
      ACC_data <- read.csv(paste(var_name[i], '1.csv', sep = ''), skip = 2)
      data <- apply(ACC_data, 1,function(x){return(sqrt(sum(x^2)))})
    }else{
      data <- unlist(read.csv(paste(var_name[i], '1.csv', sep = ''), skip = 2))
    }
    # time in seconds, start is time 0
    time <- (c(1:length(data))-1)/var_frac[i] + start
    E4[[i]] <- cbind(time, data)
    row.names(E4[[i]]) <- c()
  }
  names(E4) <- var_name
  return(E4)
}

# day2
add_time_2 <- function(session_path){
  E4 <- list()
  setwd(session_path)
  for(i in 1:length(var_name)){
    # starting time
    start <- as.numeric(unlist(strsplit(readLines(paste(var_name[i], '2.csv', sep = ''),n=1), split = ","))[1])
    if(var_name[i] == 'ACC'){
      # ACC data have 3 columns
      ACC_data <- read.csv(paste(var_name[i], '2.csv', sep = ''), skip = 2)
      data <- apply(ACC_data, 1,function(x){return(sqrt(sum(x^2)))})
    }else{
      data <- unlist(read.csv(paste(var_name[i], '2.csv', sep = ''), skip = 2))
    }
    # time in seconds, start is time 0
    time <- (c(1:length(data))-1)/var_frac[i] + start
    E4[[i]] <- cbind(time, data)
    row.names(E4[[i]]) <- c()
  }
  names(E4) <- var_name
  return(E4)
}


```


```{r}
day1 = add_time_1(session_path = session_path) 
day2 = add_time_2(session_path = session_path)

# for numeric summary in the report
test = rbind(day1[[3]], day2[[3]])
summary(test[,2])

```


### Data Preprocessing

Data Quality Control by Visualization

We did the following data quality control.

-We also deleted first 5-seond readings at the beginning of each wearing period of the device, because the device took several seconds to active and warm-up, and the redings in the warm-up period did not accurate (refer to E4 manual and support page)

```{r}
# data preoprocessing

# 1. delete 5-second readings for each wearing period (warm-up readings)
hr1 = day1[[1]] %>% as.data.frame() %>% slice(6:n())
acc1 = day1[[2]] %>%  as.data.frame() %>% slice(32*5+1:n())
temp1 = day1[[3]] %>%  as.data.frame() %>% slice(4*5+1:n())
eda1 = day1[[4]] %>%  as.data.frame() %>% slice(4*5+1:n())

hr2 = day2[[1]] %>% as.data.frame() %>% slice(6:n()) 
acc2 = day2[[2]] %>%  as.data.frame() %>% slice(32*5+1:n())
temp2 = day2[[3]] %>%  as.data.frame() %>% slice(4*5+1:n())
eda2 = day2[[4]] %>%  as.data.frame() %>% slice(4*5+1:n())

dat1 = list(hr1, acc1, temp1, eda1)
dat2 = list(hr2, acc2, temp2, eda2)

```


- Choose proper aggregation values
```{r}
# data aggregation
play_E4 <- function(session_path, slot_len, timezone){
  # read in data and add time to measured data
  setwd(session_path)
  E4 <- mapply(rbind, dat1, dat2, SIMPLIFY=FALSE)
  
  # get summaries in time slots
  E4_summary <- c()
  for(i in 1:length(var_name)){
    # the bins start from epoch time 0
    slot <- ceiling(E4[[i]][,1]/(slot_len))
    all_unique_slot <- seq(from = min(slot), to = max(slot), by = 1)
    data_slot <- matrix(NA, nrow = length(all_unique_slot), ncol = 4)
    data_slot[,1] <- all_unique_slot
    for(j in 1:length(all_unique_slot)){
      data_in_slot <- E4[[i]][slot == all_unique_slot[j],2]
      # you can calculate SD, median,...
      data_slot[j,2] <- mean(data_in_slot, na.rm = TRUE)
      data_slot[j,3] <- median(data_in_slot, na.rm = TRUE)
      data_slot[j,4] <- sd(data_in_slot, na.rm = TRUE)
    }
    colnames(data_slot) <- c('slot', paste(var_name[i],'mean', sep = '_'), 
                             paste(var_name[i],'median', sep = '_'), 
                             paste(var_name[i],'sd', sep = '_'))
    E4_summary[[i]] <- data_slot
  }
  
  # get common sampled time when all 4 types of measurements are avaliable
  slot <- E4_summary[[1]][,1]
  for(i in 2:length(var_name)){
    slot <- slot[slot %in% E4_summary[[i]][,1]]
  }
  all_slots <- sort(unique(slot))
  
  # merge data
  E4_merge <- E4_summary[[1]][E4_summary[[1]][,1] %in% all_slots,]
  for(i in 2:length(var_name)){
    E4_merge <- cbind(E4_merge, E4_summary[[i]][E4_summary[[i]][,1] %in% all_slots, -1])  
  }
  E4_merge<-as.data.frame(E4_merge)
  colnames(E4_merge) <- c('slot', paste(var_name[i],'mean', sep = '_'), 
                             paste(var_name[i],'median', sep = '_'), 
                             paste(var_name[i],'sd', sep = '_'))
  # get the relative slot w.r.t. the start of first day
  # the starting point could be different 
  # only for HR and IBI(not used here).
  # as they are derived from BVP, possibly due to adjusting time or some other reason, 
  # the derived signals start a little later than BVP and others.
  start_epoch <- min(E4[[1]][1,1], E4[[2]][1,1], E4[[3]][1,1], E4[[4]][1,1], na.rm = TRUE)
  # converge epoch time to calendar time 
  start_time <- as.POSIXlt(start_epoch, origin = "1970-01-01",tz = timezone)
  date <- substring(start_time,1,10)
  # pick the 00:00:00 of the starting day and transform it to epoch time
  ref_epoch <- as.numeric(as.POSIXct(strptime(paste(date,' 000000', sep = ""), format = '%Y-%m-%d %H%M%S', tz = timezone)))
  ref_slot <- ceiling(ref_epoch/slot_len)
  
  # the distance of observed time points to reference time (00:00:00) using the unit of 60s bin
  slot <- E4_merge[,1] - ref_slot
  # number of bins in a day
  Nslot <- 24*60*60/slot_len
  day <- ceiling(slot/Nslot) - 1
  # time of day using the unit of 1min bin
  slot_of_day <- slot %% Nslot
  slot_of_day[slot_of_day == 0] <- Nslot
  
  # get date
  days <- sort(unique(day))
  days_UTC <- ref_epoch + days*60*60*24
  days_char <- as.POSIXlt(days_UTC, origin = "1970-01-01",tz = "UTC")
  days_char <- substring(days_char,1,10)
  date <- days_char[match(day,days)]
  
  info <- as.data.frame(cbind(day, slot_of_day, E4_merge))
  info <- cbind(date, info)
  info[,3:4] <- info[,3:4]/(60*60*24/slot_len)*24
  info[,3]<-info[,3]+24*info$day
  
  colnames(info)[3:4] <- c("calendar time", "machine time")
  E4_merge <-info
  # remove NA values
  NA_flag <- apply(E4_merge,1,function(x){any(is.na(x))})
  E4_merge <- E4_merge[!NA_flag,]
  
  #transform epoch time of tags to calendar time 
  tag<-read.csv('tags.csv',header=F)
  tag_slot <- ceiling(tag/(slot_len))- ref_slot
  tag_slot_of_day <- tag_slot %% Nslot
  tags<-tag_slot_of_day/(60*60*24/slot_len)*24
  tag_day <- ceiling(tag_slot/Nslot) - 1
  tags<-tags+24*tag_day
  tags<-unique(tags)
  return(list(E4_merge,tags,start_time))
}

```


```{r}
ww = play_E4(session_path = session_path, slot_len = slot_len, timezone = timezone)

E4_info<-ww[[1]]
colnames(E4_info) = c("date", "day", "calendar time", "machine time", "HR_mean", "HR_median", "HR_sd", "ACC_mean", "ACC_median", "ACC_sd", 
"TEMP_mean", "TEMP_median", "TEMP_sd", "EDA_mean", "EDA_median", "EDA_sd")
tags<- ww[[2]]
start_time<-ww[[3]]
```


HR: mean 
We compared to the median and mean of HRreaidngs in each time slot (60 seconds), according to the boxplots and numberic summarys, there is no big difference in median and mean in terms of outliers. Therefore, we conclude both mean and median readings are robust and reliable in the dataset. We decided to use mean values in this project.

We may notice that some readigns of HR were below 60, which was not in the range of normal heart beat. However, consider to that those readigns were located during sleeping period, and the normal heart beat could be around 40- 60 during sleeping, we still consider those readings are reasonable and keep them in the data. 

```{r}
# HR
ggplot(E4_info,aes(y=HR_mean)) +
  geom_boxplot() +
  ggtitle("HR Means") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

ggplot(E4_info,aes(y=HR_median)) +
  geom_boxplot() +
  ggtitle("HR Medians") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

summary(E4_info$HR_mean)
summary(E4_info$HR_median)
```

ACC: mean
We do not use ACC data in this project. For consistency, we keep mean values of ACC. 
```{r}
# ACC
ggplot(E4_info,aes(y=ACC_mean)) +
  geom_boxplot() +
  ggtitle("ACC Means") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

ggplot(E4_info,aes(y=ACC_median)) +
  geom_boxplot() +
  ggtitle("ACC Medians") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

summary(E4_info$ACC_mean)
summary(E4_info$ACC_median)
```

Temperature: standard deviation 
However, with the boxplots of temperature mean values, we notice that most readings of body temperature were not in a normal range (i.e. 36 - 37 $\circ$C),which conflicted to the health condition during the wearing period. We noticed that the werid readings probably because the tracker was very sensitive to enviroment temperature, especially in winter Michigan, the environment temperature was pretty low, which influence the measurements of temperature a lot. Therefore, in this project, we decided do not use temperature data because of its inaccuracy.  However, it could be a good reference to track the indoor and outdoor scenarioes based on the differences in readings. Therefore, we use standard deviation of temperature readings in this project. 
```{r}
# TEMP
ggplot(E4_info,aes(y=TEMP_mean)) +
  geom_boxplot() +
  ggtitle("TEMP Means") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

ggplot(E4_info,aes(y=TEMP_median)) +
  geom_boxplot() +
  ggtitle("TEMP Medians") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

summary(E4_info$TEMP_mean)
summary(E4_info$TEMP_median)
```


EDA: standard deviation

What is Electrodermal Activity (EDA)?
 
Electrodermal activity refers to electrical changes, measured at the surface of the skin, that arise when the skin receives innervating signals from the brain. For most people, if you experience emotional activation, increased cognitive workload or physical exertion, your brain sends signals to the skin to increase the level of sweating. You may not feel any sweat on the surface of the skin, but the electrical conductance increases in a measurably significant way as the pores begin to fill below the surface.

In terms processing and understanding EDA data, we need find a baseline of EDA reading. According to E4 EDA data support page, "the best way to get a baseline now is to measure long-term, and look for the lowest smooth period of skin conductance where there is no physical movement (where the accelerometer data is smooth), and where the temperature is clearly body temperature." 

We tried to find a good baseline reading satisfied no physical movement and measuring body temperature. However, there is no reliable reading. Therefore, we decided use standard deviation of EDA to help us measure the change of EDA. 

```{r}
# EDA
ggplot(E4_info,aes(y=EDA_mean)) +
  geom_boxplot() +
  ggtitle("EDA Means") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

ggplot(E4_info,aes(y=EDA_median)) +
  geom_boxplot() +
  ggtitle("EDA Medians") +
  theme(plot.title=element_text(hjust = 0.5)) +
  theme(axis.text.x = element_blank()) 

summary(E4_info$EDA_mean)
summary(E4_info$EDA_median)

# use mean values to find baseline of EDA
# which.min(E4_info$EDA_mean)#2716
# E4_info[2716,]
```


Based on above quality checking strategies, we reduced our dataset and plot it to better understand the dataset. 

```{r}
E4_red <- E4_info[, c("date", "day", "calendar time", "HR_mean", 
                      "ACC_mean", "TEMP_sd","EDA_mean", "EDA_sd")]


```





```{r}
# plot
one_plot_with_tag<-function(dataset,tags,outcome,tag){
  time<-dataset$`calendar time`
  breaks<-round(quantile(time))
  breaks_24<-breaks%%24
  day<-breaks%/%24
  y<-unlist(select(dataset,outcome))
  if(tag==TRUE){
    p<-ggplot(dataset,aes(x=`calendar time`,y=y))+
       ylab(outcome)+
       geom_vline(xintercept = tags[1:nrow(tags),1],color='red')+
       geom_line()+
       scale_x_continuous(breaks=breaks,
                     labels =paste(paste(breaks_24,day,sep='(+'),')',sep=''))
  }else{
     p<-ggplot(dataset,aes(x=`calendar time`,y=y))+
       ylab(outcome)+
       geom_line()+
       scale_x_continuous(breaks=breaks,
                     labels =paste(paste(breaks_24,day,sep='(+'),')',sep=''))
  }
  
  return(p)
}


#eg.
p_HR_no_tag<-one_plot_with_tag(E4_red,tags,'HR_mean',tag=F)
p_HR<-one_plot_with_tag(E4_red,tags,'HR_mean',tag=T)
p_ACC<-one_plot_with_tag(E4_red,tags,'ACC_mean',tag=T)
p_EDA<-one_plot_with_tag(E4_red,tags,'EDA_mean',tag=T)
p_EDA_sd<-one_plot_with_tag(E4_red,tags,'EDA_sd',tag=F)
p_TEMP<-one_plot_with_tag(E4_info,tags,'TEMP_sd',tag=T)
grid.arrange(p_EDA, p_EDA_sd, ncol = 1)
#grid.arrange(p_HR,p_ACC,p_EDA,p_TEMP)
p_EDA
p_HR
```



```{r}
color_seg<-function(dataset, tags, outcome,tag){
  
  dataset$period<-ifelse(dataset$`calendar time`<=tags[1,1],1,nrow(tags)+1)
  for(i in 1:nrow(dataset)){
    for(j in 2:nrow(tags)){
      t<-dataset$`calendar time`[i]
      if(t<=tags[j,1]&t>tags[j-1,1])
      dataset$period[i]<-j
    }
  }
  dataset$period<-as.factor(dataset$period)
  p<-one_plot_with_tag(dataset,tags,outcome,tag)
  p_seg<-p+geom_line(aes(colour = period))+ scale_colour_identity(guide='legend')
  return(p_seg)
}

#eg.
color_seg(study, tags,'HR_mean',tag=T)
  
```









  
```{r}  

magnify<-function(mag_times,dataset,outcome,tags,tag){
  n<-ceiling(nrow(dataset)/mag_times)
  dataset$mag<-rep(1:n,each=mag_times,len=nrow(dataset))
  y<-unlist(select(dataset,outcome))
  mean_agg<-aggregate(y,list(dataset$mag),mean)
  colnames(mean_agg)<-c('No.','mean')
  time_agg<-aggregate(dataset$`calendar time`,list(dataset$mag),mean)
  colnames(time_agg)<-c('No.','time')
  data_agg<-merge(mean_agg,time_agg,by='No.')
  time<-dataset$`calendar time`
  breaks<-round(quantile(time))
  breaks_24<-breaks%%24
  day<-breaks%/%24
  p<-ggplot(data_agg,aes(x=time,y=mean))+
       ylab(outcome)+
       geom_line()+
       geom_vline(xintercept = tags[1:nrow(tags),1],color='red')+
       scale_x_continuous(breaks=breaks,
                     labels =paste(paste(breaks_24,day,sep='(+'),')',sep=''))
  return(p)
}



time<-dataset$`calendar time`
  breaks<-round(quantile(time))
  breaks_24<-breaks%%24
  day<-breaks%/%24
  y<-unlist(select(dataset,outcome))
  if(tag==TRUE){
    p<-ggplot(dataset,aes(x=`calendar time`,y=y))+
       ylab(outcome)+
       geom_vline(xintercept = tags[1:nrow(tags),1],color='red')+
       geom_line()+
       scale_x_continuous(breaks=breaks,
                     labels =paste(paste(breaks_24,day,sep='(+'),')',sep=''))
  }else{
     p<-ggplot(dataset,aes(x=`calendar time`,y=y))+
       ylab(outcome)+
       geom_line()+
       scale_x_continuous(breaks=breaks,
                     labels =paste(paste(breaks_24,day,sep='(+'),')',sep=''))
  }
  
  return(p)

#eg.
p1 = magnify(10,E4_red,'HR_mean',tags,tag=T)
p2 = magnify(10,E4_red,'EDA_mean',tags,tag=T)
grid.arrange(p1, p2, ncol = 1)

c1 = color_seg(study, tags,'HR_mean',tag=T)
c2 = color_seg(study, tags,'EDA_mean',tag=T)
c3 = color_seg(study, tags,'EDA_sd',tag=T)

grid.arrange(c1, c2,c3,  ncol = 1)
```

## Preliminary Analysis


```{r}
study1 = E4_red[(E4_red$`calendar time`>=tags[[1,1]])&(E4_red$`calendar time`<=tags[[2,1]]),]

study2 =  E4_red[(E4_red$`calendar time`>=tags[[3,1]])&(E4_red$`calendar time`<=tags[[4,1]]),]

study3 =  E4_red[(E4_red$`calendar time`>=tags[[5,1]])&(E4_red$`calendar time`<=tags[[6,1]]),]

study4 =  E4_red[(E4_red$`calendar time`>=tags[[7,1]])&(E4_red$`calendar time`<=tags[[8,1]]),]

study = rbind(study1, study2, study3, study4)



p1 = magnify(10,study1,'HR_mean',tags,tag=F)
p2 = one_plot_with_tag(study1,tags,'EDA_mean',tag=F)
p3 = one_plot_with_tag(study,tags,'EDA_sd',tag=T)

# plot for individual study periods (use one plot)

# study 1 
p1 = one_plot_with_tag(study1,tags,'HR_mean',tag=F)+ylim(60, 120) # 60 - 100
p2 = one_plot_with_tag(study1,tags,'EDA_mean',tag=F) # 0.25 - 0.75
p3 = one_plot_with_tag(study1,tags,'EDA_sd',tag=F) #0-0.6
grid.arrange(p1, p2,p3,  ncol = 1)

# study 2
p1 = one_plot_with_tag(study2,tags,'HR_mean',tag=F) + ylim(60, 120) # 60 - 100
p2 = one_plot_with_tag(study2,tags,'EDA_mean',tag=F) # 0.25 - 0.75
p3 = one_plot_with_tag(study2,tags,'EDA_sd',tag=F) #0-0.6
grid.arrange(p1, p2,p3,  ncol = 1)

# study 3
p1 = one_plot_with_tag(study3,tags,'HR_mean',tag=F) + ylim(60, 120)  +
   geom_vline(aes(xintercept= 45.965),
            color="green", linetype="dashed", size=0.5)
# 60 - 100
p2 = one_plot_with_tag(study3,tags,'EDA_mean',tag=F) +
   geom_vline(aes(xintercept= 45.965),
            color="green", linetype="dashed", size=0.5)
p3 = one_plot_with_tag(study3,tags,'EDA_sd',tag=F)  +
   geom_vline(aes(xintercept= 45.965),
            color="green", linetype="dashed", size=0.5)

grid.arrange(p1, p2,p3,  ncol = 1)

# study 4
p1 = one_plot_with_tag(study4,tags,'HR_mean',tag=F)
p2 = one_plot_with_tag(study4,tags,'EDA_mean',tag=F) # 0.25 - 0.75
p3 = one_plot_with_tag(study4,tags,'EDA_sd',tag=F) #0-0.6
grid.arrange(p1, p2,p3,  ncol = 1)

```




```{r}
eda_hr = lm(HR_mean~ EDA_mean, data =E4_red)
summary(eda_hr)

x = E4_info$HR_mean
y = E4_info$EDA_mean

plot(x, y, main = "Association of EDA and HR",
     xlab = "HR(bpm)", ylab = "EDA(μS)",
     pch = 19, frame = FALSE)
abline(lm(y ~ x, data = mtcars), col = "blue")
```

#############################################################






## Preliminary Analysis

  summary statistics, exploratoy data analysis of the wearable data and events/ episodes in personal diary


## Summary 
 what was your experience of data preprocessing? what have you enjoyed the most? What have you found to be most interesting and surprising?
 
## Discussion and Future Work
  identity three aims that you like to pursue in the next projects. Explain why you choose these three aims in the future analyses. 












```{r cars}
summary(cars)
```



You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```




References:
https://support.empatica.com/hc/en-us/articles/202028739-How-is-the-acceleration-data-formatted-in-E4-connect-



https://www.nextavenue.org/stress-and-its-adverse-effect-human-heart/

https://www.nmsba.com/buying-neuromarketing/neuromarketing-techniques/what-is-gsr-galvanic-skin-response-and-how-does-it-work

https://imotions.com/blog/galvanic-skin-response/
